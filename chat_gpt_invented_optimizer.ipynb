{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SHJewell/Lets_Do_Some_Stats/blob/master/chat_gpt_invented_optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jla0rkCm6YxW",
        "outputId": "af3ac08f-f4da-4b0f-c711-24f8c4334a1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best solution found: [0.9518506513821077, 0.014417926683716428]\n",
            "Objective value: 2.5856423309448826\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "def ackley(x):\n",
        "    \"\"\"\n",
        "    A common benchmark function:\n",
        "    f(x) = -20 * exp(-0.2 * sqrt(0.5 * (x1^2 + x2^2)))\n",
        "           - exp(0.5*(cos(2*pi*x1) + cos(2*pi*x2))) + e + 20\n",
        "    Generalizable to n dimensions:\n",
        "    \"\"\"\n",
        "    x = np.array(x)\n",
        "    n = len(x)\n",
        "    return -20.0 * math.exp(-0.2 * math.sqrt(np.sum(x**2)/n)) \\\n",
        "           - math.exp(np.sum(np.cos(2.0*math.pi*x))/n) + 20 + math.e\n",
        "\n",
        "def random_point(lower_bounds, upper_bounds, dim):\n",
        "    \"\"\" Generate a random point within the given bounds. \"\"\"\n",
        "    return [\n",
        "        random.uniform(lower_bounds[i], upper_bounds[i])\n",
        "        for i in range(dim)\n",
        "    ]\n",
        "\n",
        "def random_local_search(best_point, lower_bounds, upper_bounds, step_size=0.1):\n",
        "    \"\"\"\n",
        "    Simple local search: pick a random direction near `best_point`.\n",
        "    \"\"\"\n",
        "    dim = len(best_point)\n",
        "    candidate = []\n",
        "    for i in range(dim):\n",
        "        # Propose a new coordinate in a small neighborhood\n",
        "        perturbation = random.uniform(-step_size, step_size)\n",
        "        new_coord = best_point[i] + perturbation\n",
        "        # Clip to boundaries\n",
        "        new_coord = max(lower_bounds[i], min(upper_bounds[i], new_coord))\n",
        "        candidate.append(new_coord)\n",
        "    return candidate\n",
        "\n",
        "def global_random_search(lower_bounds, upper_bounds, dim):\n",
        "    \"\"\"\n",
        "    Pure global random sample in the domain.\n",
        "    \"\"\"\n",
        "    return random_point(lower_bounds, upper_bounds, dim)\n",
        "\n",
        "def bandit_optimizer(objective_func,\n",
        "                     dim=2,\n",
        "                     lower_bounds=None,\n",
        "                     upper_bounds=None,\n",
        "                     n_init=5,      # number of initial random points\n",
        "                     max_iter=50,   # total number of bandit steps\n",
        "                     step_size=0.1, # local search step size\n",
        "                     epsilon=0.1    # epsilon-greedy param\n",
        "                    ):\n",
        "    \"\"\"\n",
        "    A toy meta-heuristic that adaptively selects among sub-optimizers:\n",
        "    - random_local_search\n",
        "    - global_random_search\n",
        "\n",
        "    For demonstration, we only define two sub-optimizers.\n",
        "    You could add more, e.g. calling scikit-optimize or nevergrad.\n",
        "    \"\"\"\n",
        "    if lower_bounds is None:\n",
        "        lower_bounds = [-5] * dim\n",
        "    if upper_bounds is None:\n",
        "        upper_bounds = [ 5] * dim\n",
        "\n",
        "    # List of sub-optimizers (bandit arms)\n",
        "    sub_optimizers = [random_local_search, global_random_search]\n",
        "    # We'll track total improvement each sub-optimizer has given us\n",
        "    improvements = [0.0 for _ in sub_optimizers]\n",
        "    # We'll track how many times each sub-optimizer was used\n",
        "    usage_count = [1 for _ in sub_optimizers]  # start from 1 to avoid zero division\n",
        "\n",
        "    # 1) Initialization\n",
        "    best_point = None\n",
        "    best_value = float('inf')\n",
        "    dim = len(lower_bounds)\n",
        "    population = [\n",
        "        random_point(lower_bounds, upper_bounds, dim)\n",
        "        for _ in range(n_init)\n",
        "    ]\n",
        "\n",
        "    for pt in population:\n",
        "        val = objective_func(pt)\n",
        "        if val < best_value:\n",
        "            best_value = val\n",
        "            best_point = pt\n",
        "\n",
        "    # 2) Iterations\n",
        "    for iteration in range(max_iter):\n",
        "        # --- Bandit selection strategy (epsilon-greedy for simplicity) ---\n",
        "        if random.random() < epsilon:\n",
        "            # explore\n",
        "            chosen_idx = random.randint(0, len(sub_optimizers)-1)\n",
        "        else:\n",
        "            # exploit the arm with the highest average improvement\n",
        "            avg_improvements = [\n",
        "                improvements[i] / usage_count[i] for i in range(len(sub_optimizers))\n",
        "            ]\n",
        "            chosen_idx = int(np.argmax(avg_improvements))\n",
        "\n",
        "        chosen_sub_optimizer = sub_optimizers[chosen_idx]\n",
        "\n",
        "        # 3) Propose a new candidate\n",
        "        if chosen_sub_optimizer == random_local_search:\n",
        "            candidate = chosen_sub_optimizer(best_point, lower_bounds, upper_bounds, step_size)\n",
        "        else:\n",
        "            candidate = chosen_sub_optimizer(lower_bounds, upper_bounds, dim)\n",
        "\n",
        "        candidate_val = objective_func(candidate)\n",
        "\n",
        "        # 4) Update best\n",
        "        old_best = best_value\n",
        "        if candidate_val < best_value:\n",
        "            best_value = candidate_val\n",
        "            best_point = candidate\n",
        "\n",
        "        improvement = max(0.0, old_best - best_value)\n",
        "        improvements[chosen_idx] += improvement\n",
        "        usage_count[chosen_idx] += 1\n",
        "\n",
        "    return best_point, best_value\n",
        "\n",
        "def main():\n",
        "    # Example usage on Ackley function, 2D\n",
        "    best_x, best_f = bandit_optimizer(ackley, dim=2)\n",
        "    print(\"Best solution found:\", best_x)\n",
        "    print(\"Objective value:\", best_f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}